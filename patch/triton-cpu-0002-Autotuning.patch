From ef050fda49643135e50e516a830244741ad2098e Mon Sep 17 00:00:00 2001
From: cuiwenfeng <wenfeng.cui@terapines.com>
Date: Wed, 18 Sep 2024 17:33:54 +0800
Subject: [PATCH] Autotuning

---
 python/triton/runtime/autotuner.py | 63 +++++++++++++++++++-----------
 python/triton/runtime/jit.py       |  7 ++++
 third_party/cpu/backend/driver.py  | 32 ++++++++++++---
 3 files changed, 75 insertions(+), 27 deletions(-)

diff --git a/python/triton/runtime/autotuner.py b/python/triton/runtime/autotuner.py
index 33477ae1..680c287b 100644
--- a/python/triton/runtime/autotuner.py
+++ b/python/triton/runtime/autotuner.py
@@ -107,10 +107,19 @@ class Autotuner(KernelInterface):
         current = dict(meta, **config.all_kwargs())
         full_nargs = {**self.nargs, **current}
 
+        # Add shape postfix to the generated directory
+        # FIXME:
+        TUNING_SHAPE_CONFIG=""
+        # TUNING_SHAPE_CONFIG=os.getenv("ENABLE_AUTOTUNING")
+        for k in config.kwargs.keys():
+          TUNING_SHAPE_CONFIG += "_" + str(config.all_kwargs()[k])
+        os.environ["TUNING_SHAPE_CONFIG"] = TUNING_SHAPE_CONFIG
+        # print("set TUNING_SHAPE_CONFIG :", TUNING_SHAPE_CONFIG)
+
         def kernel_call():
-            if config.pre_hook:
-                config.pre_hook(full_nargs)
-            self.pre_hook(args)
+        #     if config.pre_hook:
+        #         config.pre_hook(full_nargs)
+        #     self.pre_hook(args)
             try:
                 self.fn.run(
                     *args,
@@ -123,14 +132,16 @@ class Autotuner(KernelInterface):
                     # Throw exception raised by `self.fn.run`
                     raise
 
-            self.post_hook(args, exception=None)
+            # self.post_hook(args, exception=None)
 
         try:
-            if self.use_cuda_graph:
-                return do_bench_cudagraph(kernel_call, rep=self.num_reps, quantiles=(0.5, 0.2, 0.8))
-            device = driver.active.get_current_target().backend
-            return do_bench(kernel_call, warmup=self.num_warmups, rep=self.num_reps, quantiles=(0.5, 0.2, 0.8),
-                            device_type=device)
+        #     if self.use_cuda_graph:
+        #         return do_bench_cudagraph(kernel_call, rep=self.num_reps, quantiles=(0.5, 0.2, 0.8))
+        #     device = driver.active.get_current_target().backend
+        #     return do_bench(kernel_call, warmup=self.num_warmups, rep=self.num_reps, quantiles=(0.5, 0.2, 0.8),
+        #                     device_type=device)
+            kernel_call()
+            return [float("inf"), float("inf"), float("inf")]
         except (OutOfResources, CompileTimeAssertionFailure):
             return [float("inf"), float("inf"), float("inf")]
 
@@ -162,19 +173,27 @@ class Autotuner(KernelInterface):
             config = self.cache[key]
         else:
             config = self.configs[0]
-        self.best_config = config
-        if os.getenv("TRITON_PRINT_AUTOTUNING", None) == "1" and not used_cached_result:
-            print(f"Triton autotuning for function {self.base_fn.__name__} finished after "
-                  f"{self.bench_time:.2f}s; best config selected: {self.best_config};")
-        if config.pre_hook is not None:
-            config.pre_hook({**self.nargs, **kwargs, **config.all_kwargs()})
-        ret = self.fn.run(
-            *args,
-            **kwargs,
-            **config.all_kwargs(),
-        )
-        self.nargs = None
-        return ret
+            # if 'TUNING_SHAPE_CONFIG' in os.environ:
+            #     del os.environ['TUNING_SHAPE_CONFIG']
+            os.environ.pop('TUNING_SHAPE_CONFIG', None)
+            ret = self.fn.run(
+                            *args,
+                            **kwargs,
+                            **config.all_kwargs(),
+                        )
+        # self.best_config = config
+        # if os.getenv("TRITON_PRINT_AUTOTUNING", None) == "1" and not used_cached_result:
+        #     print(f"Triton autotuning for function {self.base_fn.__name__} finished after "
+        #           f"{self.bench_time:.2f}s; best config selected: {self.best_config};")
+        # if config.pre_hook is not None:
+        #     config.pre_hook({**self.nargs, **kwargs, **config.all_kwargs()})
+        # ret = self.fn.run(
+        #     *args,
+        #     **kwargs,
+        #     **config.all_kwargs(),
+        # )
+        # self.nargs = None
+        # return ret
 
     def prune_configs(self, kwargs):
         pruned_configs = self.configs
diff --git a/python/triton/runtime/jit.py b/python/triton/runtime/jit.py
index efa844b4..153cd962 100644
--- a/python/triton/runtime/jit.py
+++ b/python/triton/runtime/jit.py
@@ -660,7 +660,14 @@ class JITFunction(KernelInterface[T]):
             self.cache[device_key][key] = kernel
 
         launcher_src_dir = os.getenv("KERNEL_AUX_FILE_DIR")
+        block_shape = os.getenv("TUNING_SHAPE_CONFIG")
+        # print("get TUNING_SHAPE_CONFIG :", block_shape)
+        if block_shape is None:
+            block_shape = ""
+
         if launcher_src_dir is not None:
+            # launcher_src_dir +="/" + block_shape
+            launcher_src_dir +=block_shape
             os.makedirs(launcher_src_dir, mode=0o777, exist_ok=True)
             llir_path = os.path.join(launcher_src_dir, kernel.name + ".llir")
             # print("llir_path: ", llir_path)
diff --git a/third_party/cpu/backend/driver.py b/third_party/cpu/backend/driver.py
index 69f8bc0d..1dec2761 100644
--- a/third_party/cpu/backend/driver.py
+++ b/third_party/cpu/backend/driver.py
@@ -24,17 +24,25 @@ libraries = ["stdc++"]
 def compile_module_from_src(inc, src, kernel_name):
     launcher_include_dir = os.getenv("KERNEL_LAUNCHER_INCLUDE_DIR")
     launcher_src_dir = os.getenv("KERNEL_AUX_FILE_DIR")
+
+
+    block_shape = os.getenv("TUNING_SHAPE_CONFIG")
+    if block_shape is None:
+        block_shape =""
+
     if launcher_include_dir is None:
        launcher_include_dir = tempfile.mkdtemp()
 
-    os.makedirs(launcher_include_dir, mode=0o777, exist_ok=True)
-
     if launcher_src_dir is None:
        launcher_src_dir = launcher_include_dir
 
+    # launcher_include_dir +="/" + block_shape
+    # launcher_src_dir +="/" + block_shape
+    # launcher_include_dir += block_shape
+    launcher_src_dir += block_shape
+    os.makedirs(launcher_include_dir, mode=0o777, exist_ok=True)
     os.makedirs(launcher_src_dir, mode=0o777, exist_ok=True)
 
-
     # print("launcher include dir: ", launcher_include_dir)
     # print("launcher src dir: ", launcher_src_dir)
     inc_path = os.path.join(launcher_include_dir, kernel_name+"_launcher.h")
@@ -121,7 +129,7 @@ def ty_to_cpp(ty):
     }[ty]
 
 
-def make_launcher(constants, signature, ids, kernel_name):
+def make_launcher(constants, signature, ids, kernel_name, constexprs_arg_names):
     # Record the end of regular arguments;
     # subsequent arguments are architecture-specific descriptors.
     arg_decls = ', '.join(f"{ty_to_cpp(ty)} arg{i}" for i, ty in signature.items())
@@ -151,6 +159,13 @@ def make_launcher(constants, signature, ids, kernel_name):
     kernel_fn_args_list = ', '.join(f"arg{i}" for i in kernel_fn_args)
     kernel_fn_arg_types = ', '.join([f"{ty_to_cpp(signature[i])}" for i in kernel_fn_args] + ["uint32_t"] * 6)
 
+    kernel_constants_declare = "".join(f"extern const int {kernel_name}_{arg_name};\n" for arg_id, arg_name in constexprs_arg_names.items() if isinstance(constants[arg_id], int) )
+    kernel_constants_definition = "".join(f"const int {kernel_name}_{arg_name} = {constants[arg_id]};\n" for arg_id, arg_name in constexprs_arg_names.items() if isinstance(constants[arg_id], int))
+
+
+    # print(kernel_constants_declare)
+    # print(kernel_constants_definition)
+
     inc = f"""
 #include <stdint.h>
 #include <cstddef>
@@ -163,6 +178,8 @@ extern "C"{{
  void({kernel_name})({kernel_fn_arg_types});
 }}
 
+{kernel_constants_declare}
+
 void {kernel_name}_omp(uint32_t gridX, uint32_t gridY, uint32_t gridZ,
                         {kernel_name}_kernel_ptr_t kernel_ptr {', ' + arg_decls if len(arg_decls) > 0 else ''});
 """
@@ -176,6 +193,8 @@ void {kernel_name}_omp(uint32_t gridX, uint32_t gridY, uint32_t gridZ,
 #include <optional>
 #include <stdio.h>
 
+{kernel_constants_definition}
+
 void {kernel_name}_omp(uint32_t gridX, uint32_t gridY, uint32_t gridZ, {kernel_name}_kernel_ptr_t kernel_ptr {', ' + arg_decls if len(arg_decls) > 0 else ''}) {{
   // TODO: Consider using omp collapse(3) clause for simplicity?
   auto all_grids = get_all_grids(gridX, gridY, gridZ);
@@ -208,9 +227,12 @@ class CPULauncher(object):
         ids = {"ids_of_const_exprs": src.fn.constexprs if hasattr(src, "fn") else tuple()}
         constants = src.constants if hasattr(src, "constants") else dict()
         cst_key = lambda i: src.fn.arg_names.index(i) if isinstance(i, str) else i
+
+        constexprs_arg_names = {cst_key(key): key for key, value in constants.items()  if(cst_key(key) in  src.fn.constexprs)}
+
         constants = {cst_key(key): value for key, value in constants.items()}
         signature = {cst_key(key): value for key, value in src.signature.items()}
-        inc, src = make_launcher(constants, signature, ids, name)
+        inc, src = make_launcher(constants, signature, ids, name, constexprs_arg_names)
         compile_module_from_src(inc, src, name)
         # self.launch = mod.launch
 
-- 
2.37.2

